{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d40538",
   "metadata": {},
   "source": [
    "# Llama 3 Fine-tuning with PyTorch FSDP and Q-Lora on Amazon SageMaker (Local Mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2f886",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70802683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"huggingface_hub[cli]\" --upgrade --quiet\n",
    "\n",
    "# Hugging Face 로그인 (실제 토큰으로 교체 필요)\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/ec2-user/SageMaker/.cache/huggingface'\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/.cache/huggingface'\n",
    "!huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533c962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "# SageMaker local 모드 설정\n",
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {'local': {'local_code': True}}\n",
    "\n",
    "role = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04405c",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059352ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove, batched=False)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "\n",
    "# 로컬 파일 시스템에 데이터셋 저장\n",
    "import os\n",
    "\n",
    "local_data_path = '/tmp/llama3_data'\n",
    "os.makedirs(f\"{local_data_path}/train\", exist_ok=True)\n",
    "os.makedirs(f\"{local_data_path}/test\", exist_ok=True)\n",
    "\n",
    "dataset[\"train\"].to_json(f\"{local_data_path}/train/dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{local_data_path}/test/dataset.json\", orient=\"records\")\n",
    "\n",
    "print(f\"Training data saved to: {local_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab112df9-c056-4fa6-9282-400083a6d2c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Template would look like this:\n",
    "\n",
    "You are a helpful Assistant. \n",
    "\n",
    "Human: What is 2+2? \n",
    "\n",
    "Assistant: 2+2 equals 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909d9f4",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama 3 on Amazon SageMaker (Local Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe91b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile llama_3_8b_fsdp_qlora.yaml\n",
    "model_id: \"meta-llama/Meta-Llama-3-8b\"\n",
    "max_seq_len: 3072\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"\n",
    "output_dir: \"/tmp/llama3\"\n",
    "report_to: \"tensorboard\"\n",
    "learning_rate: 0.0002\n",
    "lr_scheduler_type: \"constant\"\n",
    "num_train_epochs: 1\n",
    "per_device_train_batch_size: 1\n",
    "per_device_eval_batch_size: 1\n",
    "gradient_accumulation_steps: 2\n",
    "optim: adamw_torch\n",
    "logging_steps: 10\n",
    "save_strategy: epoch\n",
    "evaluation_strategy: epoch\n",
    "max_grad_norm: 0.3\n",
    "warmup_ratio: 0.03\n",
    "bf16: false\n",
    "fp16: true\n",
    "gradient_checkpointing: true\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "    backward_prefetch: \"backward_pre\"\n",
    "    forward_prefetch: \"false\"\n",
    "    use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf76a2-8e83-4efb-b57b-3d40d8cc07f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "environment={\n",
    "    # ... 기존 환경 변수들 ...\n",
    "    \"NCCL_P2P_DISABLE\": \"1\",\n",
    "    \"NCCL_IB_DISABLE\": \"1\",\n",
    "    \"NCCL_SOCKET_IFNAME\": \"lo\",\n",
    "    \"CUDA_DEVICE_ORDER\": \"PCI_BUS_ID\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3bd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = f'llama3-8b-exp1-local'\n",
    "\n",
    "# Hugging Face 토큰 (실제 토큰으로 교체 필요)\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_fsdp_qlora.py',\n",
    "    source_dir='./scripts/fsdp',\n",
    "    instance_type='local_gpu',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.36.0',\n",
    "    pytorch_version='2.1.0',\n",
    "    py_version='py310',\n",
    "    hyperparameters={\n",
    "        \"config\": \"/opt/ml/input/data/config/llama_3_8b_fsdp_qlora.yaml\"\n",
    "    },\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/home/ec2-user/SageMaker/.cache/huggingface\",\n",
    "        \"TRANSFORMERS_CACHE\": \"/home/ec2-user/SageMaker/.cache/huggingface\",\n",
    "        \"HF_HOME\": \"/home/ec2-user/SageMaker/.cache/huggingface\",\n",
    "        \"HF_TOKEN\": \"YOUR_TOKEN\",\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\", # Only for Local Instance \n",
    "        \"NCCL_SOCKET_IFNAME\": \"lo\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"NCCL_P2P_DISABLE\": \"1\",\n",
    "        \"TMPDIR\": \"/home/ec2-user/SageMaker/tmp\",\n",
    "        \"TEMP\": \"/home/ec2-user/SageMaker/tmp\",\n",
    "        \"TMP\": \"/home/ec2-user/SageMaker/tmp\",\n",
    "    },\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "\n",
    "# 절대 경로로 변경\n",
    "local_data_path = os.path.abspath('/tmp/llama3_data')\n",
    "\n",
    "data = {\n",
    "    'train': f\"file://{local_data_path}/train\",\n",
    "    'test': f\"file://{local_data_path}/test\",\n",
    "    'config': f\"file://{os.path.abspath('llama_3_8b_fsdp_qlora.yaml')}\"\n",
    "}\n",
    "\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b3efc",
   "metadata": {},
   "source": [
    "## 4. Deploy & Test fine-tuned Llama 3 on Amazon SageMaker (Local Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23910a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "config = {\n",
    "    'HF_MODEL_ID': \"/opt/ml/model\",\n",
    "    'SM_NUM_GPUS': \"1\",\n",
    "    'MAX_INPUT_LENGTH': \"8000\",\n",
    "    'MAX_TOTAL_TOKENS': \"8096\",\n",
    "    'MAX_BATCH_PREFILL_TOKENS': \"16182\",\n",
    "    'MESSAGES_API_ENABLED': \"true\",\n",
    "}\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "    model_data=huggingface_estimator.model_data,\n",
    "    role=role,\n",
    "    transformers_version=\"4.36.0\",\n",
    "    pytorch_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    env=config,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# 로컬 엔드포인트에 모델 배포\n",
    "llm = llm_model.deploy(initial_instance_count=1, instance_type='local')\n",
    "\n",
    "# 모델 테스트\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about Amazon SageMaker?\"}\n",
    "]\n",
    "\n",
    "parameters = {\n",
    "    \"model\": \"meta-llama-3-fine-tuned\",\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = llm.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())\n",
    "\n",
    "# 정리\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1307f-076c-418b-8d2b-9989d7e3f16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
